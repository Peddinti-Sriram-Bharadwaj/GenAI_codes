import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# for ssl certification
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

# Device configuration
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

class VAE(nn.Module):
    def __init__(self, latent_dim=20):
        super().__init__()

        # Encoder
        self.encoder = nn.Sequential(
                nn.Linear(28*28,512), 
                nn.ReLU(),
                nn.Linear(512,256),
                nn.ReLU(),
                nn.Linear(256, 128)
                )
        self.fc_mu = nn.Linear(128, latent_dim)
        self.fc_log_var = nn.Linear(128, latent_dim)

        self.decoder = nn.Sequential(
                nn.Linear(latent_dim, 128),
                nn.ReLU(),
                nn.Linear(128, 256),
                nn.ReLU(),
                nn.Linear(256, 512),
                nn.ReLU(),
                nn.Linear(512, 28*28),
                nn.Sigmoid()
                )

    def reparameterize(self, mu, log_var):
        std = torch.exp(log_var)
        epsilon = torch.randn_like(std)
        return mu + std * epsilon

    def forward(self, x):
        h = self.encoder(x)

        mu = self.fc_mu(h)
        log_var = self.fc_log_var(h)

        z = self.reparameterize(mu, log_var)

        reconstruction = self.decoder(z)

        return reconstruction, mu, log_var

# Hyperparamters
num_epochs = 10
learning_rate = 1e-3
batch_size = 64
latent_dim = 20

transform = transforms.ToTensor()
train_dataset = datasets.MNIST(
        root = "./data", train = True, download = True, transform = transform
        )
train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True
        )

model = VAE(latent_dim=latent_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr = learning_rate)

# VAE Loss Function
def vae_loss(recon_x, x, mu, log_var):
    recon_loss = nn.functional.binary_cross_entropy(
            recon_x, x.view(-1, 784), reduction = "sum"
            )
    kld = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())

    return recon_loss + kld

# Training loop

print("Starting VAE training")
for epoch in range(num_epochs):
    for data in train_loader:
        images, _ = data
        images = images.view(images.size(0), -1).to(device)

        recon_images, mu, log_var = model(images)

        loss = vae_loss(recon_images, images, mu, log_var)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item() / len(train_dataset):.4f}")

print("Training finished!")

print("Generating new digits from the latent space")
model.eval()

with torch.no_grad():
    z = torch.randn(64, latent_dim).to(device)


    generated_images = model.decoder(z)

    generated_images = generated_images.view(-1,1,28,28).cpu()

grid_img = torchvision.utils.make_grid(generated_images, nrow = 8)

plt.figure(figsize=(10,10))
plt.imshow(grid_img.permute(1,2,0))
plt.title("Digits Generated by the VAE")
plt.axis("off")
plt.show()
